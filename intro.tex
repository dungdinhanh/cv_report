\section{Introduction}
Diet is really important in human life. Knowing the adequate nutrition from food we eat from meals is essential for health control. To know what we eat, we often make a record of everyday meals. Such food recording is usually a manual exercise using textual description, but manual recording is tedious and time consuming. To overcome this difficulty, there have been attempts to assist food recording by using information technology. Image recognition of food items would be a good solution to food recording. Taking a picture would then be a sufficient record. However, we know that there is a wide diversity of types of food. Even within the same food category, there is considerable diversity. Therefore, despite the attempts at food item recognition, recognition performance is not yet satisfactory.


Deep learning has recently been used in image recognition. Deep learning is a collective term for algorithms having a deep architecture that solves complex problems. The most distinctive characteristic is that better image features for recognition are automatically extracted via train-ing. The convolutional neural network (CNN) is one of the methods that satisfy the requirements of the deep learning approach. CNN is now a state-of-the-art technique for image recognition challenges such as the Large Scale Visual Recognition Challenge.


In this paper, we apply CNN to the recognition and detection of food images and evaluate its performance. Our contributions are as follows: we built a dataset for food recognition experiments by using food-domain images obtained from a food logging system available for public use; we optimized CNN’s hyper parameters, showing that CNN signiﬁcantly improved the food recognition accuracy compared with a conventional method using a support vector machine (SVM) with hand-crafted features; through observation of our trained CNN, we found that color features dominate the food recognition process; we showed that CNN has signiﬁcantly better performance for the task.



The CNN offers a state-of-the-art technique for image recognition. It is a multilayer neural network, whose neuronstake small patches of the previous layer as input. It is robust against small shifts and rotations. A CNN system comprisesa convolution layer and a pooling (or subsampling) layer. In the convolution layer, unlike for general fully connected neural networks, weights can be considered as $n \times n$ ($n <input size$) ﬁlters. Each input convolves these ﬁlters. Eachlayer has many ﬁlters that generate diﬀerent outputs. For the image recognition task, the diﬀerent features are extracted by these ﬁlters. The ﬁlters are often called (convolution) kernels. The pooling layer produces the outputs by activation over rectangular regions. There are several activation methods, such as maximum activation and average activation. This makes the CNN’s outputs more invariant with respect to position. A typical CNN comprises multiple convolution and pooling layers, with a fully connected layer to produce the ﬁnalresult of the task. In image classiﬁcation, each unit of theﬁnal layer indicates the class probability.


A CNN has hyper parameters that include the number of middle layers, the size of the convolution kernels, and the active functions. In this paper, we compare the optimization of some procedures using CNN model for food classification.


First, we consider VGGNet. The runner-up at the ILSVRC 2014 competition is dubbed VGGNet by the community and was developed by Simonyan and Zisserman. VGGNet consists of 16 convolutional layers and is very appealing because of its very uniform architecture. Similar to AlexNet, only 3x3 convolutions, but lots of filters. Trained on 4 GPUs for 2–3 weeks. It is currently the most preferred choice in the community for extracting features from images. The weight configuration of the VGGNet is publicly available and has been used in many other applications and challenges as a baseline feature extractor. However, VGGNet consists of 138 million parameters, which can be a bit challenging to handle.

 
Next, Resnet, at the ILSVRC 2015, the so-called Residual Neural Network (ResNet) by Kaiming He et al introduced an ovel architecture with “skip connections” and features heavy batch normalization. Such skip connections are also known as gated units or gated recurrent units and have a strong similarity to recent successful elements applied in RNNs. Thanks to this technique they were able to train a NN with 152 layers while still having lower complexity than VGGNet. It achieves a top-5 error rate of 3.57\% which beats human-level performance on this dataset.


Squeeze-and-Excitation Networks (SENets) introduce a building block for CNNs that improves channel interdependencies at almost no computational cost. They were used at this years ImageNet competition and helped to improve the result from last year by 25\%. Besides this huge performance boost, they can be easily added to existing architectures. The main idea is this: Let’s add parameters to each channel of a convolutional block so that the network can adaptively adjust the weighting of each feature map.